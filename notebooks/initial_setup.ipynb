{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# Workflow for AI Quiz Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Setup Output Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output_structure(pdf_name, display_path=False):\n",
    "    \"\"\"\n",
    "    Create the output folder structure for a given PDF as follows:\n",
    "    ./outputs/<pdf_name>/\n",
    "        - raw_text.txt\n",
    "        - chunks.json\n",
    "        - questions.json\n",
    "        - run_metadata.json\n",
    "    \"\"\"\n",
    "    base_output = Path(\"./outputs\") / pdf_name\n",
    "    base_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    raw_text_file = base_output / \"raw_text.txt\"\n",
    "    \n",
    "    output_paths = {\n",
    "        \"base\": base_output,\n",
    "        \"raw_text\": raw_text_file,\n",
    "        \"chunks_json\": base_output / \"chunks.json\",\n",
    "        \"questions_json\": base_output / \"questions.json\",\n",
    "        \"metadata_json\": base_output / \"run_metadata.json\"\n",
    "    }\n",
    "\n",
    "    if display_path:\n",
    "        print(f\"\\nOutput structure will be created as follows: {output_paths['base']}\")\n",
    "        print(f\"  - raw_text.txt: {output_paths['raw_text']}\")\n",
    "        print(f\"  - chunks.json: {output_paths['chunks_json']}\")\n",
    "        print(f\"  - questions.json: {output_paths['questions_json']}\")\n",
    "        print(f\"  - run_metadata.json: {output_paths['metadata_json']}\")\n",
    "        print('\\n')\n",
    "    \n",
    "    return output_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Extract PDF Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path, output_paths, return_pages=False, save_pages=False):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_text = []\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        pages_text.append(text)\n",
    "    doc.close()\n",
    "\n",
    "    print(f\"Extracted {len(pages_text)} pages\")\n",
    "    print(\"--- Page 1 preview ---\")\n",
    "    print(pages_text[0][:500])\n",
    "\n",
    "    # Save extracted text to raw_text.txt\n",
    "    if save_pages:\n",
    "        raw_text_file = output_paths[\"raw_text\"]\n",
    "        with open(raw_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\\n\".join(pages_text))\n",
    "        print(f\"Saved extracted_text.txt to {raw_text_file}\")\n",
    "\n",
    "    if return_pages:\n",
    "        return pages_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Cleaning the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Minimal cleaning for PDF-extracted text.\n",
    "\n",
    "    What this does:\n",
    "    1. Unicode normalization (fixes ligatures like ﬁ, ﬀ, etc.)\n",
    "    2. Normalize newlines and whitespace\n",
    "    3. Remove standalone page numbers (e.g., '15', '203')\n",
    "\n",
    "    What this intentionally does NOT do (may implement later):\n",
    "    - No header/footer detection\n",
    "    - No paragraph restructuring\n",
    "    - No figure/table removal\n",
    "    - No heuristic guessing\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Unicode normalization (fix ligatures and weird characters)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # 2. Normalize newlines\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # 3. Normalize whitespace (keep line structure)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = \"\\n\".join(line.rstrip() for line in text.split(\"\\n\"))\n",
    "\n",
    "    # 4. Remove standalone page numbers\n",
    "    cleaned_lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        stripped = line.strip()\n",
    "        if stripped.isdigit() and len(stripped) <= 4:\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    # 5. Collapse excessive blank lines (keep max 1)\n",
    "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
    "    cleaned_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Chunking Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_fixed_size(text, output_paths, chunk_size = 4000, overlap= 400, min_chunk_size = 800, save_pages=False):\n",
    "    \"\"\"\n",
    "    Splits text into fixed-size character chunks with overlap.\n",
    "\n",
    "    Returns:\n",
    "      List of chunks:\n",
    "      {\n",
    "        \"chunk_id\": int,\n",
    "        \"text\": str,\n",
    "        \"char_len\": int\n",
    "      }\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    text_len = len(text)\n",
    "    chunk_id = 0\n",
    "    start = 0\n",
    "\n",
    "    while start < text_len:\n",
    "        end = start + chunk_size\n",
    "        chunk_text = text[start:end].strip()\n",
    "\n",
    "        if len(chunk_text) >= min_chunk_size:\n",
    "            chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_text,\n",
    "                \"char_len\": len(chunk_text)\n",
    "            })\n",
    "            chunk_id += 1\n",
    "\n",
    "        # move start forward, keeping overlap\n",
    "        start += max(1, chunk_size - overlap)\n",
    "\n",
    "    # Save chunks to JSON\n",
    "    if save_pages:\n",
    "        chunks_file = output_paths[\"chunks_json\"]\n",
    "        with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved chunks to {chunks_file}\")\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Generate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "##### Generate Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions = 10\n",
    "\n",
    "def question_prompt(chunk_text, num_questions=10):\n",
    "    '''\n",
    "    Generate a prompt for creating quiz questions from a text chunk.\n",
    "    '''\n",
    "    \n",
    "    return f\"\"\"\n",
    "    You are an expert at creating quiz questions that test deep understanding of the material. \n",
    "    Generate {num_questions} challenging quiz questions of medium difficulty based on the following except. \n",
    "    The excerpt may contain PDF artifacts (page numbers, figure labels, axis ticks, glossary/margin terms, broken line wraps).\n",
    "    Ignore non-explanatory artifacts and focus on the conceptual content. \n",
    "\n",
    "    Each question should have 4 answer options (A, B, C, D) with only one correct answer. \n",
    "    The questions should require critical thinking and not be answerable by simple keyword matching.\n",
    "\n",
    "    Generate {num_questions} total questions and return them in the following format:\n",
    "    {{\n",
    "        questions:[\n",
    "            {{\n",
    "                question_id: 1,\n",
    "                question: question_text,\n",
    "                options: ['A', 'B', 'C', 'D']\n",
    "                correct_answer: 'A'\n",
    "            }},\n",
    "\n",
    "            {{\n",
    "                question_id: 2,\n",
    "                question: question_text,\n",
    "                options: ['A', 'B', 'C', 'D']\n",
    "                correct_answer: 'C'\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Excerpt: \n",
    "    \\\"\\\"\\\"\\n{chunk_text}\\n\\\"\\\"\\\"\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Potential improvements to consider:\n",
    "- Add difficulty option later\n",
    "- Add a questions validation function later on, to validate questions and answer format generated by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Main Execution Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_file = Path(\"test_pdfs/islr_chap_2.pdf\")\n",
    "pdf_file = Path(\"test_pdfs/multi_column_research_paper.pdf\")\n",
    "pdf_name = pdf_file.stem  # Get filename without extension\n",
    "\n",
    "if pdf_file.exists():\n",
    "    # Setup output structure\n",
    "    output_paths = setup_output_structure(pdf_name, display_path=True)\n",
    "    \n",
    "    # Extract text from PDF\n",
    "    pages = extract_text_from_pdf(pdf_file, output_paths, return_pages=True, save_pages=True)\n",
    "    text_pages = \"\\n\\n\".join(pages)\n",
    "    \n",
    "    # Text cleaning\n",
    "    text_cleaned = clean_text(text_pages)\n",
    "    print('\\n' + text_cleaned[:1500])\n",
    "\n",
    "    # Text chunking\n",
    "    text_chunked = chunk_fixed_size(\n",
    "                            text_cleaned,\n",
    "                            output_paths,\n",
    "                            chunk_size=4000,\n",
    "                            overlap=400,\n",
    "                            min_chunk_size=800,\n",
    "                            save_pages=True                            \n",
    "                        )\n",
    "    print(f\"Created {len(text_chunked)} chunks\")\n",
    "\n",
    "    # Generate Questions\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"Please place your PDF at: {pdf_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Quiz_Generator-w9UIxfSy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
